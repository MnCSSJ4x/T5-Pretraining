{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6995886,"sourceType":"datasetVersion","datasetId":4021356}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n!pip install transformers \nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport torch.nn as nn \nimport torch.nn.functional as F\nfrom torch.optim import AdamW\n!pip install contractions\nimport contractions\nimport os\nimport pyarrow.parquet as pq\nimport re\nimport time\nimport gc\nfrom tqdm.notebook import tqdm\nfrom itertools import filterfalse\nfrom tqdm import trange\nfrom transformers import AutoTokenizer, BertForPreTraining,BertForMaskedLM \nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport torch\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-11-26T17:02:37.328756Z","iopub.execute_input":"2023-11-26T17:02:37.329187Z","iopub.status.idle":"2023-11-26T17:03:07.483092Z","shell.execute_reply.started":"2023-11-26T17:02:37.329154Z","shell.execute_reply":"2023-11-26T17:03:07.482152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sliding_window(row, chunk_size=509, overlap=50):\n    words = re.findall(r'\\b\\w+\\b', row['post'])\n    chunks = []\n    for i in range(0, len(words), chunk_size - overlap):\n        start = i\n        end = min(i + chunk_size, len(words))\n        chunk = ' '.join(words[start:end])\n        chunks.append(chunk)\n    return pd.DataFrame({'post': chunks})\ndef expand_contractions(sentence):\n  contractions_expanded = [contractions.fix(word) for word in sentence.split()]\n  return ' '.join(contractions_expanded)\ndef lower_case(sentence):\n  return ' '.join([word.lower() for word in sentence.split()])\ndef remove_punctuation(sentence):\n  return ' '.join([re.sub(r'[^\\w\\s]', '', word) for word in sentence.split()])\ndef preprocess(lst, process=True, min_words=20):\n  lst[:] = filterfalse(lambda x: len(x.split()) <= min_words, lst)\n  if process == True:\n    for i, sent in enumerate(lst):\n      # if len(sent.split()) <= min_words:\n      #   continue\n      lst[i] = lower_case(remove_punctuation(expand_contractions(sent)))\n  return lst\ndef set_seed(seed):\n  random.seed(seed)\n  np.random.seed(seed)\n  torch.manual_seed(seed)\n  if torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T17:03:07.484964Z","iopub.execute_input":"2023-11-26T17:03:07.485607Z","iopub.status.idle":"2023-11-26T17:03:07.523522Z","shell.execute_reply.started":"2023-11-26T17:03:07.485578Z","shell.execute_reply":"2023-11-26T17:03:07.522571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder_path = '/kaggle/input/textbooks'\nparquet_files = [f for f in os.listdir(folder_path) if f.endswith('.parquet')]\ndataframes = []\n\nfor file in parquet_files:\n    file_path = os.path.join(folder_path, file)\n    df = pq.read_table(file_path).to_pandas()\n    dataframes.append(df)\n\ndf_self_sup = pd.concat(dataframes, ignore_index=True)\ndf_self_sup.rename(columns={\"description\":\"post\"},inplace=True)\ndf_self_sup = pd.concat([sliding_window(row) for _, row in df_self_sup.iterrows()], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T17:03:07.524735Z","iopub.execute_input":"2023-11-26T17:03:07.525043Z","iopub.status.idle":"2023-11-26T17:03:13.281932Z","shell.execute_reply.started":"2023-11-26T17:03:07.525017Z","shell.execute_reply":"2023-11-26T17:03:13.280973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nset_seed(42)\nstart_time = time.time()\ntrain_sentences = preprocess(list(df_self_sup['post']), min_words = 50)\ntrain_df = pd.DataFrame([])\ntrain_df['post'] = train_sentences\ndel train_sentences, df_self_sup\ngc.collect()\n# Load the pre-trained BERT model and tokenizer\naccess_token = \"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\"\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n# model = nn.DataParallel(model).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T17:03:13.283281Z","iopub.execute_input":"2023-11-26T17:03:13.283986Z","iopub.status.idle":"2023-11-26T17:04:17.024373Z","shell.execute_reply.started":"2023-11-26T17:03:13.283951Z","shell.execute_reply":"2023-11-26T17:04:17.023606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TEST BLOCK FOR MASKING AND RECOVERY\n# for batch in dataloader:\n#     inputs = batch[\"input_ids\"].squeeze(dim=1)\n#     labels = batch[\"labels\"].squeeze(dim=1)\n\n#     # Extract values and convert tensors to lists for batch decoding\n#     input_texts = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n#     label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n#     print(\"Input:\")\n#     print(input_texts)\n\n#     print(\"Label:\")\n#     print(label_texts)\n#     break\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T17:04:17.026405Z","iopub.execute_input":"2023-11-26T17:04:17.026706Z","iopub.status.idle":"2023-11-26T17:04:17.031089Z","shell.execute_reply.started":"2023-11-26T17:04:17.026681Z","shell.execute_reply":"2023-11-26T17:04:17.030223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extra_id_tokens = tokenizer.additional_special_tokens\n# print(extra_id_tokens)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T17:04:17.032333Z","iopub.execute_input":"2023-11-26T17:04:17.032649Z","iopub.status.idle":"2023-11-26T17:04:17.043959Z","shell.execute_reply.started":"2023-11-26T17:04:17.032625Z","shell.execute_reply":"2023-11-26T17:04:17.043131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaskedLanguageModelingDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, mask_probability=0.15, max_length=512):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.mask_probability = mask_probability\n        self.max_length = max_length\n        self.special_tokens = self.tokenizer.additional_special_tokens \n    def mask_tokens(self, text):\n        tokens = text.split()\n        masked_tokens = []\n        idx = 0\n        i = 0\n        while i < len(tokens):\n            if random.random() < self.mask_probability:\n                # Replace all consecutive masked words with a single special token\n                current_special_token = self.special_tokens[idx % len(self.special_tokens)]\n                masked_tokens.append(current_special_token)\n                while i + 1 < len(tokens) and random.random() < self.mask_probability:\n                    i += 1\n            else:\n                masked_tokens.append(tokens[i])\n            i += 1\n            idx += 1\n\n        masked_text = \" \".join(masked_tokens)\n        return masked_text\n\n    def complement_tokens(self, text, masked_indices):\n        tokens = text.split()\n        complement_tokens = []\n        idx = 0\n        i = 0\n        while i < len(tokens):\n            if i in masked_indices:\n                # Replace all consecutive masked words with a single special token\n                current_special_token = self.special_tokens[idx % len(self.special_tokens)]\n                complement_tokens.append(current_special_token)\n                while i + 1 < len(tokens) and i + 1 in masked_indices:\n                    i += 1\n            else:\n                complement_tokens.append(tokens[i])\n            i += 1\n            idx += 1\n\n        complement_text = \" \".join(complement_tokens)\n        return complement_text\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data.iloc[idx]['post']\n\n        # Mask tokens with the given probability\n        masked_text = self.mask_tokens(text)\n\n        # Get the indices of masked tokens\n        masked_indices = [i for i, token in enumerate(masked_text.split()) if not token.startswith(\"<extra_id_\")]\n\n        # Create complement sentence\n        complement_text = self.complement_tokens(text, masked_indices)\n  \n        # Tokenize the masked text\n        input_ids = self.tokenizer(\n            masked_text,\n            return_tensors=\"pt\",\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True\n        ).input_ids\n\n        # Tokenize the complement text\n        labels = self.tokenizer(\n            complement_text,\n            return_tensors=\"pt\",\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True\n        ).input_ids\n\n        return {\"input_ids\": input_ids, \"labels\": labels}","metadata":{"execution":{"iopub.status.busy":"2023-11-26T17:04:17.045047Z","iopub.execute_input":"2023-11-26T17:04:17.045370Z","iopub.status.idle":"2023-11-26T17:04:17.061527Z","shell.execute_reply.started":"2023-11-26T17:04:17.045339Z","shell.execute_reply":"2023-11-26T17:04:17.060710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = MaskedLanguageModelingDataset(train_df, tokenizer)\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T17:04:17.062510Z","iopub.execute_input":"2023-11-26T17:04:17.062799Z","iopub.status.idle":"2023-11-26T17:04:17.075330Z","shell.execute_reply.started":"2023-11-26T17:04:17.062776Z","shell.execute_reply":"2023-11-26T17:04:17.074490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nlearning_rate = 5e-5\n\n# Define optimizer\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\nnum_accumulation_steps = 32\n# Training loop\nfor epoch in range(epochs):\n    loop = tqdm(dataloader, leave=True)\n    i = 0\n    for batch in loop:\n        inputs = batch[\"input_ids\"].squeeze(dim=1).to(device)\n        labels = batch[\"labels\"].squeeze(dim=1).to(device)\n        outputs = model(input_ids=inputs, labels=labels)\n        loss = outputs.loss/num_accumulation_steps\n        loss.backward()\n        if (i+1) % num_accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            loop.set_description(f'Epoch {epoch}')\n            loop.set_postfix(loss=loss.item())\n        i+=1\n# Save the trained model\nmodel.save_pretrained(\"unsupervised_t5_model\")\ntokenizer.save_pretrained(\"unsupervised_t5_model_tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2023-11-26T17:11:12.046370Z","iopub.execute_input":"2023-11-26T17:11:12.046754Z","iopub.status.idle":"2023-11-26T17:11:31.594338Z","shell.execute_reply.started":"2023-11-26T17:11:12.046725Z","shell.execute_reply":"2023-11-26T17:11:31.592481Z"},"trusted":true},"execution_count":null,"outputs":[]}]}